# SimStack - AI-Powered Simulation Orchestration Platform

**FutureStack GenAI Hackathon Project**

SimStack is an intelligent simulation orchestration platform that uses **Cerebras inference** with **Meta's Llama 3.1** models to automatically plan, spawn, and analyze parallel simulation scenarios using **Docker MCP containers**. Get actionable insights from complex what-if analyses in seconds, powered by Cerebras' industry-leading 1800+ tokens/second inference speed.

## ğŸ¯ Project Overview

SimStack solves the problem of complex operational planning by:
1. Taking high-level goals (e.g., "reduce ER wait time by 20%")
2. Using Llama 3.1 via Cerebras to generate optimal simulation plans
3. Spawning parallel Docker containers running different scenario variants
4. Streaming real-time results back to users
5. Exporting reproducible Docker Compose configurations

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     WebSocket      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   React     â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚  Go Backend  â”‚
â”‚  Dashboard  â”‚                     â”‚  (FastAPI)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                                    Cerebras API
                                    (Llama 3.1)
                                           â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â–¼                  â–¼                  â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  Queue   â”‚       â”‚ Traffic  â”‚      â”‚ Resource â”‚
                  â”‚Simulator â”‚       â”‚Simulator â”‚      â”‚Simulator â”‚
                  â”‚ (Docker) â”‚       â”‚ (Docker) â”‚      â”‚ (Docker) â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Sponsor Technology Integration

### 1. **Cerebras Cloud SDK**
- **Location**: `backend/internal/cerebras/client.go`
- **Usage**: OpenAI-compatible API client for Llama 3.1 inference
- **Speed**: Tracks tokens/sec (target: 1800+) displayed in performance metrics
- **Models**: 
  - `llama3.1-8b` for fast planning (default)
  - `llama3.1-70b` for complex reasoning (configurable)

### 2. **Meta Llama 3.1**
- **Location**: `backend/internal/orchestrator/engine.go:47-145`
- **Usage**: 
  - Tool calling with function schemas for simulator selection
  - Variant parameter generation from user goals
  - JSON-structured planning responses
- **Function Calling**: Three tool schemas defined for queue, traffic, and resource simulators

### 3. **Docker MCP Tools**
- **Location**: 
  - `simulators/queue/` - M/M/1 queueing system
  - `simulators/traffic/` - Traffic flow simulation
  - `simulators/resource/` - Staff allocation simulator
- **Implementation**: Each simulator is a FastAPI service in a Docker container with standardized `/simulate` endpoint
- **Orchestration**: Backend spawns parallel HTTP calls to simulator containers (lines 196-319)

## ğŸ“¦ Installation & Setup

### Prerequisites
- Docker & Docker Compose
- Go 1.22+ (for local development)
- Node.js 18+ (for frontend)
- Cerebras API key ([get one here](https://cloud.cerebras.ai))

### Quick Start

1. **Clone and configure environment**:
```bash
git clone <repo-url>
cd cerebrus-docker-meta
export CEREBRAS_API_KEY="your-key-here"
```

2. **Start all services**:
```bash
docker compose up --build
```

This will start:
- Backend API on `localhost:8080`
- Queue simulator on `localhost:8101`
- Traffic simulator on `localhost:8102`
- Resource simulator on `localhost:8103`

3. **Run frontend** (separate terminal):
```bash
cd frontend
npm install
npm run dev
```
Frontend runs on `localhost:5173`

## ğŸ® Usage

### Web Interface
1. Open `http://localhost:5173`
2. Enter a goal: "reduce ER wait time by 20%"
3. Click "Start"
4. Watch real-time events stream:
   - `plan` - Cerebras generates simulation variants
   - `sim_start` - Each variant begins
   - `sim_complete` - Results arrive
   - `done` - All simulations complete

### API Endpoints

**Start a simulation run**:
```bash
curl -X POST http://localhost:8080/api/run \
  -H "Content-Type: application/json" \
  -d '{"goal": "reduce ER wait time by 20%"}'
```

**Export winning scenario as Docker Compose**:
```bash
curl -X POST http://localhost:8080/api/export \
  -H "Content-Type: application/json" \
  -d '{"goal": "optimize staffing", "parameters": {"staff": 25}}' \
  -o winning-scenario.yml
```

**View performance metrics**:
```bash
curl http://localhost:8080/metrics
# Returns: {"planner_ms": 450, "simulation_startup_ms": 230, "tokens_per_second": 1850.5}
```

**WebSocket for real-time events**:
```javascript
const ws = new WebSocket('ws://localhost:8080/ws');
ws.onmessage = (e) => {
  const event = JSON.parse(e.data);
  console.log(event.type, event.payload);
};
```

## ğŸ§ª Simulator Details

### Queue Simulator
**Purpose**: Hospital ER wait times, call center queues  
**Parameters**: `arrival_rate`, `service_rate` (per hour)  
**Output**: `avg_wait_time_min`, `utilization`  
**Algorithm**: M/M/1 queueing theory

### Traffic Simulator
**Purpose**: Urban traffic planning, intersection optimization  
**Parameters**: `density` (0.0-1.0), `signal_timing` (seconds)  
**Output**: `avg_speed_kmh`, `throughput_veh_per_hr`  
**Algorithm**: Speed-density relationship

### Resource Simulator
**Purpose**: Staff scheduling, capacity planning  
**Parameters**: `staff` (count), `shifts` (array)  
**Output**: `coverage_units`, `satisfaction` (0-1)  
**Algorithm**: Linear coverage model

## ğŸ”§ Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `CEREBRAS_API_KEY` | (required) | Your Cerebras Cloud API key |
| `CEREBRAS_API_BASE` | `https://api.cerebras.ai/v1` | API endpoint |
| `CEREBRAS_MODEL` | `llama3.1-8b` | Model to use (8b/70b) |
| `SIMSTACK_ADDR` | `:8080` | Backend listen address |
| `QUEUE_SIMULATOR_URL` | `http://localhost:8101` | Queue service URL |
| `TRAFFIC_SIMULATOR_URL` | `http://localhost:8102` | Traffic service URL |
| `RESOURCE_SIMULATOR_URL` | `http://localhost:8103` | Resource service URL |

### Using Llama 3.1 70B for Complex Planning
```bash
export CEREBRAS_MODEL=llama3.1-70b
docker compose up backend
```

## ğŸ“Š Performance Monitoring

SimStack tracks key performance metrics to demonstrate Cerebras speed advantages:

- **Planner Latency**: Time for Llama 3.1 to generate simulation plan
- **Token Throughput**: Tokens per second (Cerebras target: 1800+)
- **Simulation Startup**: Time to spawn all Docker containers
- **E2E Latency**: Total time from goal to actionable results

Access metrics via `/metrics` endpoint or frontend dashboard.

## ğŸ¯ Key Features for Judging Criteria

### 1. **Potential Impact & Utility**
- Real-world applications: healthcare optimization, urban planning, workforce management
- Reduces complex analysis from hours to seconds
- Exportable, reproducible scenarios for stakeholders

### 2. **Technical Implementation**
- âœ… Cerebras Cloud SDK with OpenAI-compatible client
- âœ… Llama 3.1 function calling for tool selection
- âœ… Three fully functional Docker MCP simulators
- âœ… WebSocket real-time streaming
- âœ… Parallel execution for speed
- âœ… Go backend for performance, React for UX

### 3. **Creativity**
- Novel approach: AI plans simulations rather than humans
- Multi-tool orchestration with parallel Docker containers
- Automatic parameter variant generation
- Export-to-compose for reproducibility

### 4. **User Experience**
- Clean, modern React dashboard
- Real-time event streaming (not polling)
- One-click start with intelligent defaults
- Visual feedback for each pipeline stage
- Downloadable Docker Compose files

### 5. **Presentation Quality**
- Comprehensive documentation
- Live demo-ready setup
- Clear sponsor tech integration
- Measurable performance metrics
- Production-grade code structure

## ğŸ› ï¸ Development

### Backend Development
```bash
cd backend
go mod download
go run ./cmd/server
```

### Frontend Development
```bash
cd frontend
npm install
npm run dev
```

### Build Simulators Individually
```bash
cd simulators/queue
docker build -t simstack/queue .
docker run -p 8101:8000 simstack/queue
```

### Run Tests
```bash
# Test queue simulator
curl -X POST http://localhost:8101/simulate \
  -H "Content-Type: application/json" \
  -d '{"arrival_rate": 10, "service_rate": 12}'
```

## ğŸ“ Project Structure

```
cerebrus-docker-meta/
â”œâ”€â”€ backend/                 # Go backend service
â”‚   â”œâ”€â”€ cmd/server/         # Entry point
â”‚   â”œâ”€â”€ internal/
â”‚   â”‚   â”œâ”€â”€ cerebras/       # Cerebras API client
â”‚   â”‚   â”œâ”€â”€ orchestrator/   # Simulation orchestration engine
â”‚   â”‚   â”œâ”€â”€ server/         # HTTP + WebSocket server
â”‚   â”‚   â””â”€â”€ types/          # Shared types
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ go.mod
â”œâ”€â”€ frontend/               # React dashboard
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx        # Main component with WebSocket
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.js
â”œâ”€â”€ simulators/            # Docker MCP containers
â”‚   â”œâ”€â”€ queue/
â”‚   â”‚   â”œâ”€â”€ app.py        # FastAPI service
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â”œâ”€â”€ traffic/
â”‚   â””â”€â”€ resource/
â”œâ”€â”€ docker-compose.yml     # Full stack orchestration
â””â”€â”€ README.md             # This file
```

## ğŸš€ Demo Script

**30-Second Demo Flow**:
1. Show architecture diagram
2. Start docker-compose (already running)
3. Open frontend at `localhost:5173`
4. Enter goal: "reduce ER wait time by 20%"
5. Click Start â†’ watch real-time events
6. Highlight: Plan from Cerebras in ~450ms
7. Highlight: 3 variants running in parallel
8. Show results with metrics comparison
9. Export winning scenario as Docker Compose
10. Show `/metrics` endpoint with 1800+ tokens/sec

**Key Talking Points**:
- "Cerebras delivers plans in under 500ms - 5x faster than traditional inference"
- "Llama 3.1 function calling selects optimal simulators automatically"
- "Docker MCP ensures reproducible, isolated simulations"
- "From goal to actionable insight in seconds, not hours"

## ğŸ† Sponsor Integration Proof

### Cerebras
- File: `backend/internal/cerebras/client.go`
- Metrics: `/metrics` endpoint shows tokens/sec
- Screenshots: Performance dashboard with >1800 tok/s

### Meta Llama 3.1
- File: `backend/internal/orchestrator/engine.go:52-98`
- Function schemas for tool calling
- JSON-structured planning responses

### Docker
- Three MCP simulators in `simulators/`
- `docker-compose.yml` orchestration
- Exportable compose files from `/api/export`

## ğŸ“ License

MIT License - FutureStack Hackathon 2025

## ğŸ™ Acknowledgments

- **Cerebras** for blazing-fast inference infrastructure
- **Meta** for open-source Llama 3.1 models
- **Docker** for containerization and MCP tools

---

**Built for FutureStack GenAI Hackathon** | [Demo Video](#) | [Slides](#)

